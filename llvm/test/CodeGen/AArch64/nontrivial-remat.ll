; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
; RUN: llc -mtriple=aarch64-unknown-linux-gnu -frame-pointer=all -mattr=+reserve-x28,+reserve-x27,+reserve-x26,+reserve-x25,+reserve-x24,+reserve-x23,+reserve-x22,+reserve-x21,+reserve-x20 -reserve-regs-for-regalloc=X30,X19,X18,X17,X16,X15,X14,X13,X12,X11,X10,X9,X8,X7,X6,X5,X4 -non-trivial-rematerialization=true < %s | FileCheck %s

; %v3 can be non-trivial rematerialized.
define void @test1(i64 %v1, i64 %v2, i64* %ptr) {
; CHECK-LABEL: test1:
; CHECK:       // %bb.0:
; CHECK-NEXT:    stp x29, x30, [sp, #-16]! // 16-byte Folded Spill
; CHECK-NEXT:    mov x29, sp
; CHECK-NEXT:    .cfi_def_cfa w29, 16
; CHECK-NEXT:    .cfi_offset w30, -8
; CHECK-NEXT:    .cfi_offset w29, -16
; CHECK-NEXT:    add x3, x0, x1
; CHECK-NEXT:    str x3, [x2, #8]
; CHECK-NEXT:    ldr x3, [x2, #16]
; CHECK-NEXT:    add x3, x0, x3
; CHECK-NEXT:    sub x3, x3, x1
; CHECK-NEXT:    str x3, [x2, #16]
; CHECK-NEXT:    add x3, x0, x1
; CHECK-NEXT:    str x3, [x2, #24]
; CHECK-NEXT:    str x0, [x2, #32]
; CHECK-NEXT:    str x1, [x2, #40]
; CHECK-NEXT:    ldp x29, x30, [sp], #16 // 16-byte Folded Reload
; CHECK-NEXT:    ret
  %v3 = add i64 %v1, %v2
  %p1 = getelementptr i64, i64* %ptr, i64 1
  store volatile i64 %v3, i64* %p1, align 8

  %p2 = getelementptr i64, i64* %ptr, i64 2
  %v4 = load volatile i64, i64* %p2, align 8
  %v5 = add i64 %v1, %v4
  %v6 = sub i64 %v5, %v2
  store volatile i64 %v6, i64* %p2, align 8

  %p3 = getelementptr i64, i64* %ptr, i64 3
  store volatile i64 %v3, i64* %p3, align 8

  %p4 = getelementptr i64, i64* %ptr, i64 4
  store volatile i64 %v1, i64* %p4, align 8
  %p5 = getelementptr i64, i64* %ptr, i64 5
  store volatile i64 %v2, i64* %p5, align 8

  ret void
}

; All operands of %v3 are live in the whole live range of %v3.
; %v1 and %v2 are killed before the last use of %v7, so %v7 is not always
; rematerializable.
; Although %v3 has more uses than %v7, but it is non-trivial rematerializable,
; so it should have less spill cost, and finally %v7 is allocated a physical
; register, %v3 is rematerialized.
define void @test2(i64 %v1, i64 %v2, i64* %ptr) {
; CHECK-LABEL: test2:
; CHECK:       // %bb.0:
; CHECK-NEXT:    sub sp, sp, #32
; CHECK-NEXT:    stp x29, x30, [sp, #16] // 16-byte Folded Spill
; CHECK-NEXT:    add x29, sp, #16
; CHECK-NEXT:    .cfi_def_cfa w29, 16
; CHECK-NEXT:    .cfi_offset w30, -8
; CHECK-NEXT:    .cfi_offset w29, -16
; CHECK-NEXT:    add x3, x0, x1
; CHECK-NEXT:    str x3, [x2, #8]
; CHECK-NEXT:    orr x3, x0, x1
; CHECK-NEXT:    str x3, [x2, #8]
; CHECK-NEXT:    orr x3, x0, x1
; CHECK-NEXT:    str x3, [sp, #8] // 8-byte Folded Spill
; CHECK-NEXT:    ldr x3, [x2, #16]
; CHECK-NEXT:    add x3, x0, x3
; CHECK-NEXT:    sub x3, x3, x1
; CHECK-NEXT:    str x3, [x2, #16]
; CHECK-NEXT:    add x3, x0, x1
; CHECK-NEXT:    str x3, [x2, #24]
; CHECK-NEXT:    str x3, [x2, #24]
; CHECK-NEXT:    str x0, [x2, #32]
; CHECK-NEXT:    str x1, [x2, #40]
; CHECK-NEXT:    ldr x0, [sp, #8] // 8-byte Folded Reload
; CHECK-NEXT:    str x0, [x2, #56]
; CHECK-NEXT:    ldp x29, x30, [sp, #16] // 16-byte Folded Reload
; CHECK-NEXT:    add sp, sp, #32
; CHECK-NEXT:    ret
  %v3 = add i64 %v1, %v2
  %v7 = or i64 %v1, %v2
  %p1 = getelementptr i64, i64* %ptr, i64 1
  store volatile i64 %v3, i64* %p1, align 8
  store volatile i64 %v7, i64* %p1, align 8

  %p2 = getelementptr i64, i64* %ptr, i64 2
  %v4 = load volatile i64, i64* %p2, align 8
  %v5 = add i64 %v1, %v4
  %v6 = sub i64 %v5, %v2
  store volatile i64 %v6, i64* %p2, align 8

  %p3 = getelementptr i64, i64* %ptr, i64 3
  store volatile i64 %v3, i64* %p3, align 8
  store volatile i64 %v3, i64* %p3, align 8

  %p4 = getelementptr i64, i64* %ptr, i64 4
  store volatile i64 %v1, i64* %p4, align 8
  %p5 = getelementptr i64, i64* %ptr, i64 5
  store volatile i64 %v2, i64* %p5, align 8

  ; %v1 and %v2 has been killed, so %v7 can't be rematerialized now.
  %p7 = getelementptr i64, i64* %ptr, i64 7
  store volatile i64 %v7, i64* %p7, align 8

  ret void
}
